{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e631d919",
   "metadata": {},
   "source": [
    "# Reproducibility Notebook — AGNews & IMDB Fusion Experiments\n",
    "\n",
    "**Purpose (paper-facing):** Reproduce the experiments reported in the paper *“Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers”*.\n",
    "\n",
    "This notebook is intended to be run **top-to-bottom** without manual edits.\n",
    "\n",
    "## What this reproduces\n",
    "- Fusion operators: **Add**, **Concat+Projection**, **Gate-Scalar**\n",
    "- Dataset(s): as configured below\n",
    "- Seeds: **0–4** (paired-seed protocol where applicable)\n",
    "\n",
    "## Notes\n",
    "- Datasets are **not** included in the repository.\n",
    "- All final figures used in the paper are exported to `../results/figures/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG (single source of truth)\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "FIG_DIR = RESULTS_DIR / \"figures\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEEDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Dataset paths (edit as needed)\n",
    "AGNEWS_PATH = Path(os.environ.get(\"AGNEWS_PATH\", \"./data/agnews\"))\n",
    "IMDB_PATH   = Path(os.environ.get(\"IMDB_PATH\", \"./data/imdb\"))\n",
    "ARXIV_PATH  = Path(os.environ.get(\"ARXIV_DATA_DIR\", \"./data/arxiv\"))\n",
    "\n",
    "# Training hyperparameters (must match paper)\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "LR = 3e-4\n",
    "\n",
    "# Model hyperparameters\n",
    "D_MODEL = 256\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "DIM_FF = 1024\n",
    "DROPOUT = 0.1\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"CONFIG loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6227acc",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777e3de",
   "metadata": {},
   "source": [
    "# Phase 1 — Replication Runs (AG News + IMDB)  \n",
    "**Sinusoidal PE only** · Fusion: **add / concat / gate** · **5 seeds** · **resume-safe** · **best-epoch test evaluation**\n",
    "\n",
    "This notebook is the next step after Arxiv: reproduce the same fusion comparison on **IMDB** and **AG News** using the **same fusion equations** as your Arxiv notebook:\n",
    "\n",
    "- `add`: `E + P`  \n",
    "- `concat`: `Linear([E;P])`  \n",
    "- `gate`: `sigmoid(Linear([E;P])) * E + (1-gate) * P`\n",
    "\n",
    "Key properties:\n",
    "- Saves a **last checkpoint** every epoch (for resume).\n",
    "- Saves a separate **best checkpoint** (by validation accuracy).\n",
    "- Final reported test metrics are computed on the **best checkpoint** (Phase‑1 style).\n",
    "- AG News loader is fixed for your **headerless** 3‑column CSV.\n",
    "\n",
    "Outputs written to `./pefusion_phase1_runs/`:\n",
    "- `checkpoints_last/` and `checkpoints_best/`\n",
    "- `logs/*.jsonl`\n",
    "- `results_runs.csv` (one row per run)\n",
    "- `results_agg.csv` (aggregated over seeds)\n",
    "- `length_stats.csv` (dataset length distributions & clipping rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, math, time, random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Repro / deterministic-ish\n",
    "# ----------------------------\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_seed = (torch.initial_seed() + worker_id) % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca85d44",
   "metadata": {},
   "source": [
    "## Main experiment code\n",
    "Run top-to-bottom. Edit only CONFIG above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Config (EDIT PATHS IF NEEDED)\n",
    "# ----------------------------\n",
    "CONFIG = {\n",
    "    # DATA PATHS\n",
    "    \"imdb_dir\": \"/zeng_gk/Amine/Imdb_Data\",            # train.csv, validation.csv, test.csv with columns text,label\n",
    "    \"agnews_dir\": \"/zeng_gk/Amine/AG_News Data\",       # train.csv, test.csv (headerless: label,title,description)\n",
    "\n",
    "    # OUTPUT\n",
    "    \"out_dir\": \"./pefusion_phase1_runs\",\n",
    "\n",
    "    # EXPERIMENT GRID\n",
    "    \"datasets\": [\"imdb\", \"agnews\"],\n",
    "    \"pe_type\": \"sinusoidal\",\n",
    "    \"fusions\": [\"add\", \"concat\", \"gate\"],\n",
    "    \"seeds\": [0, 1, 2, 3, 4],\n",
    "\n",
    "    # MODEL\n",
    "    \"vocab_max_size\": 50000,\n",
    "    \"vocab_min_freq\": 2,\n",
    "    \"max_len\": 512,\n",
    "    \"d_model\": 256,\n",
    "    \"nhead\": 8,\n",
    "    \"num_layers\": 4,\n",
    "    \"dim_ff\": 1024,\n",
    "    \"dropout\": 0.1,\n",
    "\n",
    "    # TRAINING\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"grad_clip\": 1.0,\n",
    "\n",
    "    # TIMING (latency, synthetic)\n",
    "    \"timing_warmup\": 5,\n",
    "    \"timing_repeats\": 10,\n",
    "\n",
    "    # Optional early stop (0 disables)\n",
    "    \"early_stop_patience\": 0,\n",
    "\n",
    "    # Dataloader\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "CONFIG['lr']=LR\n",
    "CONFIG['max_len']=MAX_LEN\n",
    "\n",
    "os.makedirs(CONFIG[\"out_dir\"], exist_ok=True)\n",
    "print(\"Output dir:\", os.path.abspath(CONFIG[\"out_dir\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7428e",
   "metadata": {},
   "source": [
    "## Data loading (your exact formats)\n",
    "\n",
    "- IMDB: `text`, `label`\n",
    "- AG News: headerless 3 cols → `(label, title, description)`, text = title + description, label mapped to 0..3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a84178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_imdb_from_dir(dir_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    tr_path = os.path.join(dir_path, \"train.csv\")\n",
    "    va_path = os.path.join(dir_path, \"validation.csv\")\n",
    "    te_path = os.path.join(dir_path, \"test.csv\")\n",
    "    if not (os.path.exists(tr_path) and os.path.exists(va_path) and os.path.exists(te_path)):\n",
    "        raise FileNotFoundError(f\"IMDB: expected train.csv, validation.csv, test.csv in {dir_path}\")\n",
    "\n",
    "    train_df = pd.read_csv(tr_path)\n",
    "    val_df   = pd.read_csv(va_path)\n",
    "    test_df  = pd.read_csv(te_path)\n",
    "\n",
    "    for df, name in [(train_df, \"train\"), (val_df, \"validation\"), (test_df, \"test\")]:\n",
    "        if not set([\"text\",\"label\"]).issubset(df.columns):\n",
    "            raise ValueError(f\"IMDB {name}: expected columns ['text','label'], got {list(df.columns)}\")\n",
    "\n",
    "    return train_df[[\"text\",\"label\"]].copy(), val_df[[\"text\",\"label\"]].copy(), test_df[[\"text\",\"label\"]].copy()\n",
    "\n",
    "def load_agnews_from_dir(dir_path: str, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_path = os.path.join(dir_path, \"train.csv\")\n",
    "    test_path  = os.path.join(dir_path, \"test.csv\")\n",
    "    if not (os.path.exists(train_path) and os.path.exists(test_path)):\n",
    "        raise FileNotFoundError(f\"AG News: expected train.csv and test.csv in {dir_path}\")\n",
    "\n",
    "    cols = [\"label\",\"title\",\"description\"]\n",
    "    train_df = pd.read_csv(train_path, header=None, names=cols)\n",
    "    test_df  = pd.read_csv(test_path,  header=None, names=cols)\n",
    "\n",
    "    def norm_label(x):\n",
    "        x = int(x)\n",
    "        return x - 1 if x in [1,2,3,4] else x\n",
    "\n",
    "    for df in [train_df, test_df]:\n",
    "        df[\"label\"] = df[\"label\"].apply(norm_label)\n",
    "        df[\"text\"] = (df[\"title\"].astype(str) + \" \" + df[\"description\"].astype(str)).astype(str)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(train_df))\n",
    "    rng.shuffle(idx)\n",
    "    split = int(0.9 * len(train_df))\n",
    "    tr_idx, va_idx = idx[:split], idx[split:]\n",
    "\n",
    "    train = train_df.iloc[tr_idx][[\"text\",\"label\"]].reset_index(drop=True)\n",
    "    val   = train_df.iloc[va_idx][[\"text\",\"label\"]].reset_index(drop=True)\n",
    "    test  = test_df[[\"text\",\"label\"]].reset_index(drop=True)\n",
    "    return train, val, test\n",
    "\n",
    "# Smoke-check\n",
    "im_tr, im_va, im_te = load_imdb_from_dir(CONFIG[\"imdb_dir\"])\n",
    "ag_tr, ag_va, ag_te = load_agnews_from_dir(CONFIG[\"agnews_dir\"], seed=0)\n",
    "print(\"IMDB:\", len(im_tr), len(im_va), len(im_te), \"labels(train)=\", sorted(im_tr[\"label\"].unique()))\n",
    "print(\"AG  :\", len(ag_tr), len(ag_va), len(ag_te), \"labels(train)=\", sorted(ag_tr[\"label\"].unique()))\n",
    "print(\"AG example text:\", ag_tr[\"text\"].iloc[0][:140])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8934a",
   "metadata": {},
   "source": [
    "## Tokenization, vocab, encoding\n",
    "\n",
    "Baseline: lowercase + whitespace split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14156e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "def token_len(text: str) -> int:\n",
    "    return len(simple_tokenize(text))\n",
    "\n",
    "def build_vocab(texts: List[str], min_freq: int, max_size: int) -> Dict[str, int]:\n",
    "    freq: Dict[str,int] = {}\n",
    "    for t in texts:\n",
    "        for tok in simple_tokenize(t):\n",
    "            freq[tok] = freq.get(tok, 0) + 1\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    words = [w for w, c in freq.items() if c >= min_freq]\n",
    "    words.sort(key=lambda w: -freq[w])\n",
    "    for w in words[: max_size - len(vocab)]:\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(text: str, vocab: Dict[str,int], max_len: int) -> List[int]:\n",
    "    toks = simple_tokenize(text)\n",
    "    ids = [vocab.get(t, vocab[\"<unk>\"]) for t in toks[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [vocab[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], vocab: Dict[str,int], max_len: int):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = torch.tensor(encode(self.texts[idx], self.vocab, self.max_len), dtype=torch.long)\n",
    "        y = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def make_dataloaders(dataset_name: str, seed: int):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    if dataset_name == \"imdb\":\n",
    "        train_df, val_df, test_df = load_imdb_from_dir(CONFIG[\"imdb_dir\"])\n",
    "        num_classes = 2\n",
    "    elif dataset_name == \"agnews\":\n",
    "        train_df, val_df, test_df = load_agnews_from_dir(CONFIG[\"agnews_dir\"], seed=seed)\n",
    "        num_classes = 4\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "    train_texts = train_df[\"text\"].tolist()\n",
    "    train_labels = train_df[\"label\"].tolist()\n",
    "    val_texts = val_df[\"text\"].tolist()\n",
    "    val_labels = val_df[\"label\"].tolist()\n",
    "    test_texts = test_df[\"text\"].tolist()\n",
    "    test_labels = test_df[\"label\"].tolist()\n",
    "\n",
    "    vocab = build_vocab(train_texts, CONFIG[\"vocab_min_freq\"], CONFIG[\"vocab_max_size\"])\n",
    "\n",
    "    train_ds = TextClsDataset(train_texts, train_labels, vocab, CONFIG[\"max_len\"])\n",
    "    val_ds   = TextClsDataset(val_texts, val_labels, vocab, CONFIG[\"max_len\"])\n",
    "    test_ds  = TextClsDataset(test_texts, test_labels, vocab, CONFIG[\"max_len\"])\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True,\n",
    "        num_workers=CONFIG[\"num_workers\"], pin_memory=(device.type==\"cuda\"),\n",
    "        worker_init_fn=worker_init_fn, generator=g\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "        num_workers=CONFIG[\"num_workers\"], pin_memory=(device.type==\"cuda\"),\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "        num_workers=CONFIG[\"num_workers\"], pin_memory=(device.type==\"cuda\"),\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        \"num_classes\": num_classes,\n",
    "        \"vocab_size\": len(vocab),\n",
    "        \"train_n\": len(train_ds),\n",
    "        \"val_n\": len(val_ds),\n",
    "        \"test_n\": len(test_ds),\n",
    "    }\n",
    "    return train_loader, val_loader, test_loader, vocab, meta, (train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24240b46",
   "metadata": {},
   "source": [
    "## Model (matches Arxiv fusion equations exactly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc532f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sinusoidal_pe(max_len: int, d_model: int, device: torch.device):\n",
    "    pe = torch.zeros(max_len, d_model, device=device)\n",
    "    position = torch.arange(0, max_len, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device) * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe  # [L, D]\n",
    "\n",
    "class PEFuse(nn.Module):\n",
    "    def __init__(self, d_model: int, fusion: str):\n",
    "        super().__init__()\n",
    "        assert fusion in [\"add\", \"concat\", \"gate\"]\n",
    "        self.fusion = fusion\n",
    "        if fusion == \"concat\":\n",
    "            self.fuse_layer = nn.Linear(d_model * 2, d_model)\n",
    "        elif fusion == \"gate\":\n",
    "            self.fuse_gate = nn.Linear(d_model * 2, d_model)\n",
    "\n",
    "    def forward(self, E: torch.Tensor, P: torch.Tensor) -> torch.Tensor:\n",
    "        if P.dim() == 2:\n",
    "            P = P.unsqueeze(0).expand(E.size(0), -1, -1)\n",
    "\n",
    "        if self.fusion == \"add\":\n",
    "            return E + P\n",
    "        elif self.fusion == \"concat\":\n",
    "            return self.fuse_layer(torch.cat([E, P], dim=-1))\n",
    "        elif self.fusion == \"gate\":\n",
    "            gate = torch.sigmoid(self.fuse_gate(torch.cat([E, P], dim=-1)))\n",
    "            return gate * E + (1 - gate) * P\n",
    "        else:\n",
    "            return E + P\n",
    "\n",
    "class TransformerTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_classes: int, d_model: int, nhead: int,\n",
    "                 num_layers: int, dim_ff: int, dropout: float, max_len: int, fusion: str):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.register_buffer(\"pe_cpu\", sinusoidal_pe(max_len, d_model, device=torch.device(\"cpu\")), persistent=False)\n",
    "        self.fuse = PEFuse(d_model, fusion)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x_ids: torch.Tensor):\n",
    "        E = self.emb(x_ids) * math.sqrt(self.d_model)  # [B,L,D]\n",
    "        pad_mask = (x_ids == 0)  # [B,L]\n",
    "        P = self.pe_cpu.to(E.device)  # [L,D]\n",
    "        E = self.fuse(E, P)\n",
    "        H = self.encoder(E, src_key_padding_mask=pad_mask)  # [B,L,D]\n",
    "\n",
    "        nonpad = (~pad_mask).unsqueeze(-1)\n",
    "        H = H * nonpad\n",
    "        denom = nonpad.sum(dim=1).clamp(min=1)\n",
    "        pooled = H.sum(dim=1) / denom\n",
    "\n",
    "        return self.cls(self.dropout(pooled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_id(dataset: str, fusion: str, seed: int) -> str:\n",
    "    return f\"{dataset}_pe=sinusoidal_fusion={fusion}_seed={seed}\"\n",
    "\n",
    "def ensure_dirs():\n",
    "    for d in [\"checkpoints_last\", \"checkpoints_best\", \"logs\"]:\n",
    "        os.makedirs(os.path.join(CONFIG[\"out_dir\"], d), exist_ok=True)\n",
    "\n",
    "def paths_for_run(dataset: str, fusion: str, seed: int) -> Dict[str,str]:\n",
    "    ensure_dirs()\n",
    "    rid = run_id(dataset, fusion, seed)\n",
    "    return {\n",
    "        \"rid\": rid,\n",
    "        \"ckpt_last\": os.path.join(CONFIG[\"out_dir\"], \"checkpoints_last\", rid + \".pt\"),\n",
    "        \"ckpt_best\": os.path.join(CONFIG[\"out_dir\"], \"checkpoints_best\", rid + \".pt\"),\n",
    "        \"log\":       os.path.join(CONFIG[\"out_dir\"], \"logs\", rid + \".jsonl\"),\n",
    "        \"runs_csv\":  os.path.join(CONFIG[\"out_dir\"], \"results_runs.csv\"),\n",
    "    }\n",
    "\n",
    "def results_csv_has_run(runs_csv: str, rid: str) -> bool:\n",
    "    if not os.path.exists(runs_csv):\n",
    "        return False\n",
    "    df = pd.read_csv(runs_csv)\n",
    "    return (df[\"run_id\"] == rid).any()\n",
    "\n",
    "def append_row(csv_path: str, row: Dict):\n",
    "    df = pd.DataFrame([row])\n",
    "    if os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "def jsonl_append(path: str, record: Dict):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_n = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = ce(logits, y)\n",
    "        b = y.size(0)\n",
    "        total_loss += loss.item() * b\n",
    "        total_correct += (logits.argmax(-1) == y).sum().item()\n",
    "        total_n += b\n",
    "    return total_loss / max(total_n,1), total_correct / max(total_n,1)\n",
    "\n",
    "def measure_latency_ms(model: nn.Module, vocab_size: int, max_len: int, batch_size: int) -> float:\n",
    "    model.eval()\n",
    "    x = torch.randint(low=1, high=max(2, vocab_size), size=(batch_size, max_len), device=device)\n",
    "    for _ in range(CONFIG[\"timing_warmup\"]):\n",
    "        _ = model(x)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(CONFIG[\"timing_repeats\"]):\n",
    "        _ = model(x)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    return ((t1 - t0) / CONFIG[\"timing_repeats\"]) * 1000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_run(dataset: str, fusion: str, seed: int):\n",
    "    p = paths_for_run(dataset, fusion, seed)\n",
    "\n",
    "    if results_csv_has_run(p[\"runs_csv\"], p[\"rid\"]):\n",
    "        print(\"[SKIP]\", p[\"rid\"])\n",
    "        return\n",
    "\n",
    "    seed_everything(seed)\n",
    "\n",
    "    train_loader, val_loader, test_loader, vocab, meta, _ = make_dataloaders(dataset, seed)\n",
    "    model = TransformerTextClassifier(\n",
    "        vocab_size=meta[\"vocab_size\"],\n",
    "        num_classes=meta[\"num_classes\"],\n",
    "        d_model=CONFIG[\"d_model\"],\n",
    "        nhead=CONFIG[\"nhead\"],\n",
    "        num_layers=CONFIG[\"num_layers\"],\n",
    "        dim_ff=CONFIG[\"dim_ff\"],\n",
    "        dropout=CONFIG[\"dropout\"],\n",
    "        max_len=CONFIG[\"max_len\"],\n",
    "        fusion=fusion,\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_epoch = 1\n",
    "    best_val_acc = -1.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    if os.path.exists(p[\"ckpt_last\"]):\n",
    "        ckpt = torch.load(p[\"ckpt_last\"], map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        opt.load_state_dict(ckpt[\"opt\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        best_val_acc = ckpt.get(\"best_val_acc\", -1.0)\n",
    "        best_epoch = ckpt.get(\"best_epoch\", 0)\n",
    "        print(f\"[RESUME] {p['rid']} from epoch {start_epoch} (best_val_acc={best_val_acc:.4f} @ epoch {best_epoch})\")\n",
    "\n",
    "    patience = CONFIG[\"early_stop_patience\"]\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(start_epoch, CONFIG[\"epochs\"] + 1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_n = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = ce(logits, y)\n",
    "            loss.backward()\n",
    "            if CONFIG[\"grad_clip\"] and CONFIG[\"grad_clip\"] > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"grad_clip\"])\n",
    "            opt.step()\n",
    "            b = y.size(0)\n",
    "            total_loss += loss.item() * b\n",
    "            total_n += b\n",
    "\n",
    "        train_loss = total_loss / max(total_n,1)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        jsonl_append(p[\"log\"], {\n",
    "            \"run_id\": p[\"rid\"], \"dataset\": dataset, \"pe_type\": \"sinusoidal\", \"fusion\": fusion, \"seed\": seed,\n",
    "            \"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": val_acc,\n",
    "            \"epoch_time_sec\": dt\n",
    "        })\n",
    "        print(f\"{p['rid']} | epoch {epoch:02d}/{CONFIG['epochs']} | {dt:.1f}s | train_loss={train_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "        improved = val_acc > best_val_acc\n",
    "        if improved:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            bad = 0\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"opt\": opt.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"best_val_acc\": best_val_acc,\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"config\": CONFIG,\n",
    "                \"meta\": meta,\n",
    "            }, p[\"ckpt_best\"])\n",
    "        else:\n",
    "            if patience and patience > 0:\n",
    "                bad += 1\n",
    "\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"opt\": opt.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": CONFIG,\n",
    "            \"meta\": meta,\n",
    "        }, p[\"ckpt_last\"])\n",
    "\n",
    "        if patience and patience > 0 and bad >= patience:\n",
    "            print(f\"[EARLY STOP] patience={patience} reached at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    ckpt_path = p[\"ckpt_best\"] if os.path.exists(p[\"ckpt_best\"]) else p[\"ckpt_last\"]\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    latency_ms = measure_latency_ms(model, vocab_size=meta[\"vocab_size\"], max_len=CONFIG[\"max_len\"], batch_size=CONFIG[\"batch_size\"])\n",
    "\n",
    "    append_row(p[\"runs_csv\"], {\n",
    "        \"run_id\": p[\"rid\"], \"dataset\": dataset, \"pe_type\": \"sinusoidal\", \"fusion\": fusion, \"seed\": seed,\n",
    "        \"epochs_configured\": CONFIG[\"epochs\"],\n",
    "        \"best_val_acc\": best_val_acc, \"best_epoch\": best_epoch,\n",
    "        \"test_acc_at_best\": test_acc, \"test_loss_at_best\": test_loss,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"vocab_size\": meta[\"vocab_size\"], \"max_len\": CONFIG[\"max_len\"],\n",
    "        \"d_model\": CONFIG[\"d_model\"], \"nhead\": CONFIG[\"nhead\"], \"num_layers\": CONFIG[\"num_layers\"],\n",
    "        \"dim_ff\": CONFIG[\"dim_ff\"], \"dropout\": CONFIG[\"dropout\"],\n",
    "        \"batch_size\": CONFIG[\"batch_size\"], \"lr\": CONFIG[\"lr\"],\n",
    "        \"ckpt_used\": os.path.basename(ckpt_path),\n",
    "    })\n",
    "    print(\"[DONE]\", p[\"rid\"], f\"test_acc_at_best={test_acc:.4f} latency_ms={latency_ms:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072ab64",
   "metadata": {},
   "source": [
    "## Length statistics (writes `length_stats.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e469fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def length_stats_for_split(df: pd.DataFrame, split_name: str, dataset: str) -> Dict:\n",
    "    lens = df[\"text\"].astype(str).apply(token_len)\n",
    "    max_len = CONFIG[\"max_len\"]\n",
    "    clipped = (lens > max_len).mean()\n",
    "    return {\n",
    "        \"dataset\": dataset,\n",
    "        \"split\": split_name,\n",
    "        \"n\": int(len(df)),\n",
    "        \"len_mean\": float(lens.mean()),\n",
    "        \"len_median\": float(lens.median()),\n",
    "        \"len_p90\": float(lens.quantile(0.90)),\n",
    "        \"len_p95\": float(lens.quantile(0.95)),\n",
    "        \"len_p99\": float(lens.quantile(0.99)),\n",
    "        \"clip_rate_at_max_len\": float(clipped),\n",
    "        \"max_len\": int(max_len),\n",
    "    }\n",
    "\n",
    "def compute_and_save_length_stats():\n",
    "    rows = []\n",
    "    tr, va, te = load_imdb_from_dir(CONFIG[\"imdb_dir\"])\n",
    "    rows += [length_stats_for_split(tr,\"train\",\"imdb\"),\n",
    "             length_stats_for_split(va,\"val\",\"imdb\"),\n",
    "             length_stats_for_split(te,\"test\",\"imdb\")]\n",
    "\n",
    "    tr, va, te = load_agnews_from_dir(CONFIG[\"agnews_dir\"], seed=0)\n",
    "    rows += [length_stats_for_split(tr,\"train\",\"agnews\"),\n",
    "             length_stats_for_split(va,\"val\",\"agnews\"),\n",
    "             length_stats_for_split(te,\"test\",\"agnews\")]\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    path = os.path.join(CONFIG[\"out_dir\"], \"length_stats.csv\")\n",
    "    out.to_csv(path, index=False)\n",
    "    display(out)\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "compute_and_save_length_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380aa3e",
   "metadata": {},
   "source": [
    "## Run the grid (resume-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_full_grid():\n",
    "    ensure_dirs()\n",
    "    runs_csv = os.path.join(CONFIG[\"out_dir\"], \"results_runs.csv\")\n",
    "\n",
    "    total = 0\n",
    "    skipped = 0\n",
    "    for dataset in CONFIG[\"datasets\"]:\n",
    "        for fusion in CONFIG[\"fusions\"]:\n",
    "            for seed in CONFIG[\"seeds\"]:\n",
    "                total += 1\n",
    "                if results_csv_has_run(runs_csv, run_id(dataset, fusion, seed)):\n",
    "                    skipped += 1\n",
    "\n",
    "    print(f\"Planned runs: {total} | already completed: {skipped} | to run now: {total - skipped}\")\n",
    "\n",
    "    for dataset in CONFIG[\"datasets\"]:\n",
    "        for fusion in CONFIG[\"fusions\"]:\n",
    "            for seed in CONFIG[\"seeds\"]:\n",
    "                print(\"=\"*100)\n",
    "                print(\"RUN:\", run_id(dataset, fusion, seed))\n",
    "                print(\"=\"*100)\n",
    "                train_one_run(dataset, fusion, seed)\n",
    "\n",
    "run_full_grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37ed72",
   "metadata": {},
   "source": [
    "## Aggregate over seeds (writes `results_agg.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cf081",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runs_path = os.path.join(CONFIG[\"out_dir\"], \"results_runs.csv\")\n",
    "if not os.path.exists(runs_path):\n",
    "    print(\"No results_runs.csv found yet. Run the grid first.\")\n",
    "else:\n",
    "    df = pd.read_csv(runs_path)\n",
    "    df = df[(df[\"pe_type\"]==\"sinusoidal\") & (df[\"fusion\"].isin(CONFIG[\"fusions\"])) & (df[\"dataset\"].isin(CONFIG[\"datasets\"]))]\n",
    "\n",
    "    agg = (df.groupby([\"dataset\",\"fusion\"])\n",
    "             .agg(\n",
    "                 runs=(\"run_id\",\"count\"),\n",
    "                 mean_test_acc=(\"test_acc_at_best\",\"mean\"),\n",
    "                 std_test_acc=(\"test_acc_at_best\",\"std\"),\n",
    "                 mean_best_val_acc=(\"best_val_acc\",\"mean\"),\n",
    "                 std_best_val_acc=(\"best_val_acc\",\"std\"),\n",
    "                 mean_latency_ms=(\"latency_ms\",\"mean\"),\n",
    "                 std_latency_ms=(\"latency_ms\",\"std\"),\n",
    "             )\n",
    "             .reset_index()\n",
    "          )\n",
    "    display(agg.sort_values([\"dataset\",\"mean_test_acc\"], ascending=[True, False]))\n",
    "\n",
    "    out_path = os.path.join(CONFIG[\"out_dir\"], \"results_agg.csv\")\n",
    "    agg.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac1cb4-64de-4b44-a02d-f1c8649501e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"interrupt worked\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b731e0-baa8-4cbf-bb12-d70b0c3caa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = \"pefusion_phase1_runs\"  # adjust if needed\n",
    "RESULTS_CSV = os.path.join(OUT_DIR, \"results_runs.csv\")\n",
    "\n",
    "def load_done(results_csv):\n",
    "    if not os.path.exists(results_csv) or os.path.getsize(results_csv) == 0:\n",
    "        return set()\n",
    "    df = pd.read_csv(results_csv, header=None)\n",
    "    header = df.iloc[0].tolist()\n",
    "    data = df.iloc[1:].copy()\n",
    "    data.columns = header\n",
    "\n",
    "    # DONE = (dataset, pe_type, fusion, seed)\n",
    "    done = set(zip(\n",
    "        data[\"dataset\"].astype(str),\n",
    "        data[\"pe_type\"].astype(str),\n",
    "        data[\"fusion\"].astype(str),\n",
    "        data[\"seed\"].astype(int),\n",
    "    ))\n",
    "    return done\n",
    "\n",
    "done = load_done(RESULTS_CSV)\n",
    "\n",
    "DATASET = \"agnews\"\n",
    "PE_TYPE = \"sinusoidal\"\n",
    "FUSIONS = [\"add\", \"concat\", \"gate\"]\n",
    "SEEDS = [0,1,2,3,4]\n",
    "\n",
    "todo = []\n",
    "for fusion in FUSIONS:\n",
    "    for seed in SEEDS:\n",
    "        key = (DATASET, PE_TYPE, fusion, seed)\n",
    "        if key not in done:\n",
    "            todo.append({\"dataset\": DATASET, \"pe_type\": PE_TYPE, \"fusion\": fusion, \"seed\": seed})\n",
    "\n",
    "print(\"DONE keys:\", len(done))\n",
    "print(\"TO RUN:\", len(todo))\n",
    "print(todo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e2201-589c-4367-8f45-df66889b1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIX: define run_id() exactly as the rest of the notebook expects ---\n",
    "def run_id(dataset: str, fusion: str, seed: int, pe_type: str = \"sinusoidal\") -> str:\n",
    "    # Must match how you name checkpoints and how run_id appears in results_runs.csv\n",
    "    return f\"{dataset}_pe={pe_type}_fusion={fusion}_seed={seed}\"\n",
    "\n",
    "# Sanity check\n",
    "print(run_id(\"agnews\", \"gate\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919d4af-5576-4f3e-a517-d636e8530cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg in todo:\n",
    "    rid_str = f\"{cfg['dataset']}_pe=sinusoidal_fusion={cfg['fusion']}_seed={cfg['seed']}\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"RUN:\", rid_str)\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    train_one_run(cfg[\"dataset\"], cfg[\"fusion\"], int(cfg[\"seed\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74ef76-2f8c-4e85-a3d2-04107bc89602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2593716",
   "metadata": {},
   "source": [
    "## Export (paper-facing artifacts)\n",
    "The following cell **must** write the final figures and (optionally) CSV summaries\n",
    "to `../results/` so they correspond exactly to the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# EXPORT\n",
    "# =========================\n",
    "# Wire your actual result variables here.\n",
    "\n",
    "# Example:\n",
    "# df.to_csv(RESULTS_DIR / \"table1_agnews_imdb.csv\", index=False)\n",
    "# plt.savefig(FIG_DIR / \"figX_agnews_imdb.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "print(\"Export cell executed. Ensure outputs are written to ../results/.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
