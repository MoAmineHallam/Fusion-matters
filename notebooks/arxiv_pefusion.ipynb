{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d776ff9",
   "metadata": {},
   "source": [
    "# Reproducibility Notebook — ArXiv PE-Fusion Experiments\n",
    "\n",
    "**Purpose (paper-facing):** Reproduce the ArXiv fusion experiments reported in the paper *“Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers”*.\n",
    "\n",
    "This notebook is intended to be run **top-to-bottom** without manual edits.\n",
    "\n",
    "## What this reproduces\n",
    "- Fusion operators: **Add**, **Concat+Projection**, **Gate-Scalar** (and **Gate-CNN** if included in this notebook)\n",
    "- Seeds: **0–4** (paired-seed comparisons)\n",
    "- Outputs exported to `../results/`:\n",
    "  - `results/table1_arxiv.csv` (or similar)\n",
    "  - figure files used in the paper (e.g., paired-seed deltas)\n",
    "\n",
    "## Notes\n",
    "- Datasets are **not** included in the repository. You must download/prepare them as described in `docs/REPRODUCIBILITY.md`.\n",
    "- If you change any hyperparameter, you must treat results as *new* and update the paper figures/tables accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG (single source of truth)\n",
    "# =========================\n",
    "# Edit ONLY this cell to change experiment settings.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Output directory (repo-relative)\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "FIG_DIR = RESULTS_DIR / \"figures\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seeds used in the paper (paired-seed protocol)\n",
    "SEEDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Dataset paths (adjust to your machine)\n",
    "# Example placeholders — keep them explicit.\n",
    "ARXIV_DATA_DIR = Path(os.environ.get(\"ARXIV_DATA_DIR\", \"./data/arxiv\"))  # change as needed\n",
    "\n",
    "# Core training hyperparameters (must match the paper settings)\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Model hyperparameters (must match the paper settings)\n",
    "D_MODEL = 256\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "DIM_FF = 1024\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Sequence constraints\n",
    "MAX_LEN = 4096\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"CONFIG loaded.\")\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR.resolve())\n",
    "print(\"FIG_DIR:\", FIG_DIR.resolve())\n",
    "print(\"ARXIV_DATA_DIR:\", ARXIV_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a60fab",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f338f44-eb57-4ed2-b514-db15a9762f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a55a8d-504b-417d-b1d7-c1c0daaf6945",
   "metadata": {},
   "source": [
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20b2f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============== Environment Setup ===============\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# =============== Reproducibility ===============\n",
    "def set_seed(seed: int):\n",
    "    import os, random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Determinism (can slow down a bit; keep ON for research reproducibility)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    # Ensures each DataLoader worker has a deterministic RNG stream\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Multi-seed study configuration\n",
    "SEEDS = [0, 1, 2, 3, 4]   \n",
    "\n",
    "# =============== Device Info ===============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "    print(f\"Memory Cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa82bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "def build_vocab(texts: List[str], min_freq: int = 2, max_size: int = 50000) -> Dict[str, int]:\n",
    "    freq = {}\n",
    "    for t in texts:\n",
    "        for tok in simple_tokenize(t):\n",
    "            freq[tok] = freq.get(tok, 0) + 1\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    words = [w for w, c in freq.items() if c >= min_freq]\n",
    "    words.sort(key=lambda w: -freq[w])\n",
    "    for w in words[: max_size - len(vocab)]:\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(text, vocab, max_len):\n",
    "    toks = simple_tokenize(text)\n",
    "    ids = [vocab.get(tok, vocab[\"<unk>\"]) for tok in toks][:max_len]\n",
    "    pad = [vocab[\"<pad>\"]] * max(0, max_len - len(ids))\n",
    "    return ids + pad\n",
    "\n",
    "class TextClsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(encode(self.texts[idx], self.vocab, self.max_len))\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPE(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class LearnedPE(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        return self.pe(pos)\n",
    "\n",
    "class RotaryPE(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.size(1), device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i , j -> ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n",
    "        return emb.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "\n",
    "class RelativePE(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.rel_embed = nn.Parameter(torch.randn(max_len, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return self.rel_embed[:seq_len].unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformerPEF(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, layers, dim_ff, dropout, pe_type, fusion, max_len, num_classes):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe_type = pe_type\n",
    "        self.fusion = fusion\n",
    "        self.d_model = d_model\n",
    "\n",
    "        if pe_type == \"sinusoidal\":\n",
    "            self.pe_module = SinusoidalPE(d_model, max_len)\n",
    "        elif pe_type == \"learned\":\n",
    "            self.pe_module = LearnedPE(max_len, d_model)\n",
    "        elif pe_type == \"rope\":\n",
    "            self.pe_module = RotaryPE(d_model)\n",
    "        elif pe_type == \"relative\":\n",
    "            self.pe_module = RelativePE(d_model, max_len)\n",
    "\n",
    "        if fusion == \"concat\":\n",
    "            self.fuse_layer = nn.Linear(d_model * 2, d_model)\n",
    "        elif fusion == \"gate\":\n",
    "            self.fuse_gate = nn.Linear(d_model * 2, 1)\n",
    "        elif fusion == \"mlp\":\n",
    "            self.fuse_mlp = nn.Sequential(\n",
    "                nn.Linear(d_model * 2, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_model, d_model)\n",
    "            )\n",
    "\n",
    "        self.tr = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True),\n",
    "            num_layers=layers\n",
    "        )\n",
    "        self.cls = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def fuse(self, E, P):\n",
    "        # Ensure shapes match for concatenation-based fusion\n",
    "        if P.size(0) != E.size(0):\n",
    "            P = P.expand(E.size(0), -1, -1)\n",
    "\n",
    "        if self.fusion == \"add\":\n",
    "            return E + P\n",
    "        elif self.fusion == \"mul\":\n",
    "            return E * P\n",
    "        elif self.fusion == \"concat\":\n",
    "            return self.fuse_layer(torch.cat([E, P], dim=-1))\n",
    "        elif self.fusion == \"gate\":\n",
    "            gate = torch.sigmoid(self.fuse_gate(torch.cat([E, P], dim=-1)))\n",
    "            return gate * E + (1 - gate) * P\n",
    "        elif self.fusion == \"mlp\":\n",
    "            return self.fuse_mlp(torch.cat([E, P], dim=-1))\n",
    "        else:\n",
    "            return E + P\n",
    "    def forward(self, x):\n",
    "        E = self.embed(x) * math.sqrt(self.d_model)\n",
    "        P = self.pe_module(x).to(x.device)\n",
    "        h = self.fuse(E, P)\n",
    "        out = self.tr(h, src_key_padding_mask=(x == 0))\n",
    "        mask = (x != 0).unsqueeze(-1).float()\n",
    "        pooled = (out * mask).sum(1) / mask.sum(1).clamp(min=1.0)\n",
    "        return self.cls(pooled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabbe06",
   "metadata": {},
   "source": [
    "## Main experiment code\n",
    "Run top-to-bottom. Do not edit cells in the middle; only edit CONFIG above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425eadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Reliable training / evaluation utilities =================\n",
    "import time, statistics\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_epoch(model, loader, optim, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    start = time.time()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return time.time() - start\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_latency(model, loader, device, repeats=10, warmup=5):\n",
    "    \"\"\"\n",
    "    Stable latency (ms/sample) using full-loader passes.\n",
    "    - warmup: number of full passes ignored\n",
    "    - repeats: number of measured full passes\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # warmup passes\n",
    "    for _ in range(warmup):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    for _ in range(repeats):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        n = 0\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "            n += x.size(0)\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        times.append(((t1 - t0) * 1000.0) / max(n, 1))\n",
    "\n",
    "    return float(np.mean(times)), float(statistics.pstdev(times))\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, optimizer, device,\n",
    "                              max_epochs=20, patience=4, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Trains up to max_epochs and keeps the best checkpoint by validation accuracy.\n",
    "    Returns: best_val_acc, best_epoch_index, best_state_dict (CPU tensors).\n",
    "    \"\"\"\n",
    "    best_val = -1.0\n",
    "    best_epoch = -1\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(max_epochs):\n",
    "        ep_time = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_acc = float(eval_acc(model, val_loader, device))\n",
    "        print(f\"Epoch {ep+1}/{max_epochs} - {ep_time:.1f}s - val_acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val + min_delta:\n",
    "            best_val = val_acc\n",
    "            best_epoch = ep\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if bad >= patience:\n",
    "            print(f\"Early stopping at epoch {ep+1}. Best epoch: {best_epoch+1} (val_acc={best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "    return best_val, best_epoch, best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd09c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Dataset + loaders + final multi-seed experiment loop =================\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------------------- Load dataset --------------------\n",
    "ds = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"data/train_clean.jsonl\",\n",
    "    \"validation\": \"data/validation_clean.jsonl\",\n",
    "    \"test\": \"data/test_clean.jsonl\"\n",
    "})\n",
    "ds = ds.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"train examples: {len(ds['train'])}\")\n",
    "print(f\"validation examples: {len(ds['validation'])}\")\n",
    "print(f\"test examples: {len(ds['test'])}\")\n",
    "\n",
    "all_texts = list(ds[\"train\"][\"text\"]) + list(ds[\"validation\"][\"text\"]) + list(ds[\"test\"][\"text\"])\n",
    "vocab = build_vocab(all_texts, min_freq=2)\n",
    "\n",
    "# -------------------- Experiment config --------------------\n",
    "args = {\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"d_model\": 128,\n",
    "    \"nhead\": 8,\n",
    "    \"layers\": 2,\n",
    "    \"dim_ff\": 256,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch\": 64,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": LR,\n",
    "    \"num_classes\": 11,\n",
    "\n",
    "    # latency: your old value=3 is too noisy; use 10/5 to keep runtime reasonable\n",
    "    \"timing_repeats\": 10,\n",
    "    \"timing_warmup\": 5,\n",
    "}\n",
    "\n",
    "def make_loaders(seed: int):\n",
    "    # DataLoader generator controls shuffle deterministically\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_ds = TextClsDataset(ds[\"train\"][\"text\"], ds[\"train\"][\"label\"], vocab, args[\"max_len\"])\n",
    "    val_ds   = TextClsDataset(ds[\"validation\"][\"text\"], ds[\"validation\"][\"label\"], vocab, args[\"max_len\"])\n",
    "    test_ds  = TextClsDataset(ds[\"test\"][\"text\"], ds[\"test\"][\"label\"], vocab, args[\"max_len\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args[\"batch\"],\n",
    "        shuffle=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "        num_workers=0,      # keep 0 for strict determinism; increase later if needed\n",
    "        pin_memory=(device.type == \"cuda\")\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=args[\"batch\"],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=(device.type == \"cuda\")\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=args[\"batch\"],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=(device.type == \"cuda\")\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# -------------------- Grid --------------------\n",
    "fusion_methods = [\"add\", \"mul\", \"concat\", \"gate\", \"mlp\"]\n",
    "pe_types = [\"sinusoidal\", \"learned\", \"rope\", \"relative\"]\n",
    "\n",
    "EARLY_STOP_PATIENCE = 4\n",
    "EARLY_STOP_MIN_DELTA = 1e-4\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "rows = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    set_seed(seed)\n",
    "    train_loader, val_loader, test_loader = make_loaders(seed)\n",
    "\n",
    "    for pe in pe_types:\n",
    "        for fusion in fusion_methods:\n",
    "            key = f\"Arxiv|pe={pe}|fusion={fusion}|seed={seed}\"\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(key)\n",
    "\n",
    "            try:\n",
    "                # Important: re-seed before model init so init is seed-controlled\n",
    "                set_seed(seed)\n",
    "\n",
    "                model = TextTransformerPEF(\n",
    "                    vocab_size=len(vocab),\n",
    "                    d_model=args[\"d_model\"],\n",
    "                    nhead=args[\"nhead\"],\n",
    "                    layers=args[\"layers\"],\n",
    "                    dim_ff=args[\"dim_ff\"],\n",
    "                    dropout=args[\"dropout\"],\n",
    "                    pe_type=pe,\n",
    "                    fusion=fusion,\n",
    "                    max_len=args[\"max_len\"],\n",
    "                    num_classes=args[\"num_classes\"],\n",
    "                ).to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "                n_params = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"Params={n_params:,}\")\n",
    "\n",
    "                # Train with early stopping (best checkpoint by validation)\n",
    "                best_val, best_epoch, best_state = train_with_early_stopping(\n",
    "                    model, train_loader, val_loader, optimizer, device,\n",
    "                    max_epochs=args[\"epochs\"],\n",
    "                    patience=EARLY_STOP_PATIENCE,\n",
    "                    min_delta=EARLY_STOP_MIN_DELTA\n",
    "                )\n",
    "\n",
    "                # Restore best checkpoint before final test + latency\n",
    "                if best_state is not None:\n",
    "                    model.load_state_dict(best_state)\n",
    "\n",
    "                # Eval\n",
    "                val_acc = float(eval_acc(model, val_loader, device))\n",
    "                test_acc = float(eval_acc(model, test_loader, device))\n",
    "\n",
    "                # Latency\n",
    "                lat_mean, lat_std = measure_latency(\n",
    "                    model, test_loader, device,\n",
    "                    repeats=args[\"timing_repeats\"],\n",
    "                    warmup=args[\"timing_warmup\"]\n",
    "                )\n",
    "\n",
    "                rows.append({\n",
    "                    \"dataset\": \"ArxivClassification\",\n",
    "                    \"pe_type\": pe,\n",
    "                    \"fusion\": fusion,\n",
    "                    \"seed\": int(seed),\n",
    "                    \"params\": int(n_params),\n",
    "                    \"best_val_acc\": float(best_val),\n",
    "                    \"best_epoch\": int(best_epoch),\n",
    "                    \"val_acc\": float(val_acc),\n",
    "                    \"test_acc\": float(test_acc),\n",
    "                    \"lat_ms_mean\": float(lat_mean),\n",
    "                    \"lat_ms_std\": float(lat_std),\n",
    "                })\n",
    "\n",
    "                # Optional: save checkpoint per seed/config\n",
    "                torch.save(model.state_dict(), f\"saved_models/{key.replace('|','_')}.pt\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Skipping {key}: {e}\")\n",
    "                rows.append({\n",
    "                    \"dataset\": \"ArxivClassification\",\n",
    "                    \"pe_type\": pe,\n",
    "                    \"fusion\": fusion,\n",
    "                    \"seed\": int(seed),\n",
    "                    \"error\": str(e),\n",
    "                })\n",
    "\n",
    "# -------------------- Save raw + aggregated results --------------------\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"arxiv_results_by_seed.csv\", index=False)\n",
    "with open(\"arxiv_results_by_seed.json\", \"w\") as f:\n",
    "    json.dump(rows, f, indent=2)\n",
    "\n",
    "df_ok = df[df.get(\"error\").isna()] if \"error\" in df.columns else df.copy()\n",
    "\n",
    "agg = (\n",
    "    df_ok.groupby([\"pe_type\", \"fusion\"], as_index=False)\n",
    "      .agg(\n",
    "          params_mean=(\"params\", \"mean\"),\n",
    "          val_acc_mean=(\"val_acc\", \"mean\"),\n",
    "          val_acc_std=(\"val_acc\", \"std\"),\n",
    "          test_acc_mean=(\"test_acc\", \"mean\"),\n",
    "          test_acc_std=(\"test_acc\", \"std\"),\n",
    "          lat_ms_mean_mean=(\"lat_ms_mean\", \"mean\"),\n",
    "          lat_ms_mean_std=(\"lat_ms_mean\", \"std\"),\n",
    "          seeds=(\"seed\", \"nunique\"),\n",
    "      )\n",
    "      .sort_values(\"test_acc_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "agg.to_csv(\"arxiv_results_aggregated.csv\", index=False)\n",
    "print(\"\\nSaved:\")\n",
    "print(\"- arxiv_results_by_seed.csv / .json (raw per seed)\")\n",
    "print(\"- arxiv_results_aggregated.csv (mean/std across seeds)\")\n",
    "display(agg.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc204a99-3ca1-4314-b299-20a5c3b39472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9ec6996",
   "metadata": {},
   "source": [
    "## Export checklist (must be true before you claim reproduction)\n",
    "- [ ] Table-1 style summary CSV written to `../results/`\n",
    "- [ ] Paired-seed delta figure(s) written to `../results/figures/`\n",
    "- [ ] Notebook runs top-to-bottom on a clean kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# EXPORT (paper-facing artifacts)\n",
    "# =========================\n",
    "# This cell is a *template*.\n",
    "# Replace variable names below with the actual objects created by your training/eval code.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Example expected objects:\n",
    "# - results_rows: list[dict] with keys like {\"seed\":0, \"fusion\":\"add\", \"acc\":0.6123}\n",
    "# - fig objects already saved during plotting OR saved here\n",
    "\n",
    "# ---- 1) Save scalar results (Table 1 inputs / deltas)\n",
    "# if 'results_rows' in globals():\n",
    "#     df = pd.DataFrame(results_rows)\n",
    "#     out_csv = RESULTS_DIR / \"table1_arxiv_runs.csv\"\n",
    "#     df.to_csv(out_csv, index=False)\n",
    "#     print(\"Wrote:\", out_csv)\n",
    "# else:\n",
    "#     print(\"NOTE: results_rows not found. Wire your results into this export cell.\")\n",
    "\n",
    "# ---- 2) Save aggregated summary (mean ± std)\n",
    "# if 'results_rows' in globals():\n",
    "#     df = pd.DataFrame(results_rows)\n",
    "#     summary = df.groupby(\"fusion\")[\"acc\"].agg([\"mean\",\"std\"]).reset_index()\n",
    "#     out_csv = RESULTS_DIR / \"table1_arxiv_summary.csv\"\n",
    "#     summary.to_csv(out_csv, index=False)\n",
    "#     print(\"Wrote:\", out_csv)\n",
    "\n",
    "# ---- 3) Ensure figures are saved to FIG_DIR\n",
    "# Example:\n",
    "# plt.savefig(FIG_DIR / \"fig3_paired_seed_deltas.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "print(\"Export cell: wire your in-notebook variables to the saves above.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
